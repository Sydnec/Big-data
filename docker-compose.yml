version: '3.8'

services:
  # PostgreSQL for Airflow metadata
  postgres-airflow:
    image: postgres:15
    container_name: postgres-airflow
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_airflow_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - bigdata-network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 30s

  # PostgreSQL for Weather data warehouse
  postgres-weather:
    image: postgres:15
    container_name: postgres-weather
    environment:
      POSTGRES_USER: ${PG_WEATHER_USER:-weather}
      POSTGRES_PASSWORD: ${PG_WEATHER_PASSWORD:-weather123}
      POSTGRES_DB: ${PG_WEATHER_DB:-weather_db}
    volumes:
      - postgres_weather_data:/var/lib/postgresql/data
      - ./init-scripts/postgres:/docker-entrypoint-initdb.d
    ports:
      - "5433:5432"
    networks:
      - bigdata-network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "weather", "-d", "weather_db"]
      interval: 10s
      retries: 5
      start_period: 10s

  # Airflow initialization
  airflow-init:
    image: apache/airflow:2.8.0
    container_name: airflow-init
    depends_on:
      postgres-airflow:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - _AIRFLOW_DB_MIGRATE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=${_AIRFLOW_WWW_USER_USERNAME}
      - _AIRFLOW_WWW_USER_PASSWORD=${_AIRFLOW_WWW_USER_PASSWORD}
      - _PIP_ADDITIONAL_REQUIREMENTS=${_PIP_ADDITIONAL_REQUIREMENTS}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
    networks:
      - bigdata-network
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin || true

  # Airflow webserver
  airflow-webserver:
    image: apache/airflow:2.8.0
    container_name: airflow-webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres-airflow:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - _PIP_ADDITIONAL_REQUIREMENTS=${_PIP_ADDITIONAL_REQUIREMENTS}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    networks:
      - bigdata-network
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Airflow scheduler
  airflow-scheduler:
    image: apache/airflow:2.8.0
    container_name: airflow-scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres-airflow:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - _PIP_ADDITIONAL_REQUIREMENTS=${_PIP_ADDITIONAL_REQUIREMENTS}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - bigdata-network
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # MongoDB for raw data storage
  mongodb:
    image: mongo
    container_name: mongodb
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
    volumes:
      - mongodb_data:/data/db
    ports:
      - "27017:27017"
    networks:
      - bigdata-network
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s

  # Mongo Express (MongoDB UI)
  mongo-express:
    image: mongo-express
    container_name: mongo-express
    depends_on:
      mongodb:
        condition: service_healthy
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: ${ME_CONFIG_MONGODB_ADMINUSERNAME}
      ME_CONFIG_MONGODB_ADMINPASSWORD: ${ME_CONFIG_MONGODB_ADMINPASSWORD}
      ME_CONFIG_MONGODB_URL: ${ME_CONFIG_MONGODB_URL}
      ME_CONFIG_BASICAUTH_USERNAME: ${ME_CONFIG_BASICAUTH_USERNAME}
      ME_CONFIG_BASICAUTH_PASSWORD: ${ME_CONFIG_BASICAUTH_PASSWORD}
    ports:
      - "8081:8081"
    networks:
      - bigdata-network

  # Spark Master
  spark-master:
    image: apache/spark:3.5.3
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8082
    ports:
      - "8082:8082"
      - "7077:7077"
    volumes:
      - ./spark/jobs:/opt/spark-jobs
      - ./spark/jars:/opt/spark-jars
    networks:
      - bigdata-network
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      --host spark-master
      --port 7077
      --webui-port 8082
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Spark Worker
  spark-worker:
    image: apache/spark:3.5.3
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_WEBUI_PORT=8083
    ports:
      - "8083:8083"
    volumes:
      - ./spark/jobs:/opt/spark-jobs
      - ./spark/jars:/opt/spark-jars
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - bigdata-network
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
      --webui-port 8083
      --cores 2
      --memory 2g

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus
    container_name: prometheus
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - bigdata-network
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s

  # Grafana for dashboards
  grafana:
    image: grafana/grafana
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GF_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_ADMIN_PASSWORD:-admin123}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards
    ports:
      - "3000:3000"
    depends_on:
      prometheus:
        condition: service_healthy
    networks:
      - bigdata-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Metrics Exporter for Prometheus
  metrics-exporter:
    build:
      context: ./metrics-exporter
      dockerfile: Dockerfile
    container_name: metrics-exporter
    environment:
      - MONGO_URI=mongodb://admin:admin@mongodb:27017/
      - MONGO_DB=weather
      - MONGO_COLLECTION=raw_weather
      - PG_HOST=postgres-weather
      - PG_PORT=5432
      - PG_USER=${PG_WEATHER_USER:-weather}
      - PG_PASSWORD=${PG_WEATHER_PASSWORD:-weather123}
      - PG_DATABASE=${PG_WEATHER_DB:-weather_db}
      - METRICS_PORT=9101
      - SCRAPE_INTERVAL=15
    ports:
      - "9101:9101"
    depends_on:
      mongodb:
        condition: service_healthy
      postgres-weather:
        condition: service_healthy
    networks:
      - bigdata-network
    restart: unless-stopped

networks:
  bigdata-network:
    driver: bridge

volumes:
  postgres_airflow_data:
  postgres_weather_data:
  mongodb_data:
  prometheus_data:
  grafana_data:
